---
title: "퍼셉트론과 인공신경망"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

{{< include ../latexmacros.qmd >}}

</br>

## 퍼셉트론

### 퍼셉트론 {#sec-ML_ann_perceptron}

앞서의 [일반화된 선형 모델](classification.qmd#sec-ML_classification_generalized_linear_model) 은 원래 생물의 신경 세포(neuron) 을 수학적으로 모사한 것이다. 로지스틱 회귀에서는 입력변수와 매개변수의 내적에 대한 시그모이드 함수로 처리하여 출력했으며, 다중 클래스 로지스틱 회귀에서는 소프트 맥스 함수를 사용하였다. 이 퍼셉트론을 여러 층으로 쌓은 것을 신경망(neural network) 이라고 하며 현재 인공지능 시스템의 핵심이 되었다. 여기서 입력변수와 매개변수의 내적을 처리하는 함수를 **활성화 함수(activation function)** 라고 하며 신경망 에서는 시그모이드나 소프트 맥스 이외에 각각의 장점과 목적에 맞게 다양한 함수를 사용 할 수 있다.

![퍼셉트론](figure/perceptron01.png){#fig-perceptron_perceptron width=400}

요약하자면

&emsp;($1$) $n$ 개의 입력 변수 $\bf{x}\in \R^n$,

&emsp;($2$) $n+1$ 개의 내부 파라미터 : 1개의 bias $b$ 와 $n$ 개의 값을 갖는 $\bf{w}=\begin{bmatrix} w_1 & \ldots &w_n\end{bmatrix}^T$,

&emsp;($3$) 활성화 함수(activation function) $\sigma(z)$ : 비선형 함수.

로 정의된다. 

</br>

## 다층 신경망

### 다층 신경망 {#sec-ML_ANN_neural_network}

![다층 신경망](figure/perceptron02.png){#fig-perceptron_multi_layer_perceptron width=600}

- **입력층(input layer)** 는 퍼셉트론이 아닌 입력 데이터 벡터이다.
- 입력층과 출력층을 제외한 신경망의 각 층을 **은닉층(hidden layer)** 라고 한다.
- 첫번째 은닉층(hidden layer)의 퍼셉트론을 $h_1^{(1)},\,h_2^{(1)},\ldots$ 와 같이 표기한다.
- $k$ 번째 은닉층의 퍼셉트론을 $h_1^{(k)},\,h_2^{(k)},\ldots$ 와 같이 표기한다.
- $k$ 번째 은닉층의 퍼셉트론의 갯수를 $N_k$ 로 표기한다.
- 출력층은 출력값 벡터를 출력하는 마지막 층의 퍼셉트론이다.
- 신경망의 층수는 입력층을 제외하고 은닉층의 수에 출력층 1 을 더하여 표기한다. 즉 위의 그림은 4층 신경망이다.

다층 퍼셉트론은 입력값 벡터 $\bf{x}\in \R^D$ 에 대해 출력값 $\hat{\bf{y}}\in \R^K$ 를 내는 함수로 내부에 각각의 퍼셉트론의 매개변수 전체 신경망의 매개변수로 가지는 함수이다. 


입력층 $\bf{x} = \begin{bmatrix}x_1 & \cdots & x_D\end{bmatrix}^T$ 에 대해 첫번째 은닉층의 $j$ 번째 퍼셉트론의 내부 파라미터 벡터를 $\bf{w}^{(1j)}$ 와 $b^{(1)}_j$ 로 표기한다면 활성화 함수 $\sigma$ 에 대해 출력값 $y^{(1)}_j$ 는 
$$
z^{(1)}_j = \sigma \left(\bf{w}^{(1j)} \bf{\cdot x}+b^{(1)}_j\right)
$$

이다. 첫번째 은닉층에 $N_1$ 개의 퍼셉트론이 있다고 하고 행렬 $\bf{W}^{(1)}$ 와 $\tilde{\bf{x}}$ 를 다음과 같이 정의하자. 

$$
\bf{W}^{(1)} := \begin{bmatrix} w^{(11)}_1 & w^{(11)}_2 & \cdots & w^{(11)}_n & b^{(1)}_1 \\ w^{(12)}_1 & w^{(12)}_2 & \cdots & w^{(12)}_n & b^{(1)}_2 \\ \vdots & & & & \vdots \\ w^{(1,N_1)}_1 & w^{(1,N_1)}_2 & \cdots & w^{(1,N_1)}_n & b^{(1)}_{N_1} \end{bmatrix},\qquad \tilde{\bf{x}} := \begin{bmatrix} x_1 \\ \vdots \\ x_n \\ 1\end{bmatrix}.
$$ {#eq-ML_parameters_representations}

즉 $\bf{W}^{(1)}\bf{\tilde{x}}$ 의 $j$ 번째 행은 첫번째 은닉층의 $j$ 번째 퍼셉트론의 활성화 함수에 적용되는 값이다. 첫번째 은닉층의 $j$ 번째 퍼셉트론의 출력값을 $z^{(j)}$ 라고 하고 

$$
\bf{z}^{(1)} = \begin{bmatrix} z^{(1)}_1 \\ \vdots \\ z^{(1)}_{N_1}\end{bmatrix} = \begin{bmatrix} \sigma_1\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{1}\right) \\ \vdots \\  \sigma_1\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{N_1}\right)  \end{bmatrix}
$$

라고 하면 $\bf{z}^{(1)}$ 은 첫번째 은닉층에서의 전체 출력값을 나타내는 벡터이다. 여기서 $\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_k$ 는 $\bf{W}^{(1)}\bf{\tilde{x}}$ 의 $k$ 행 값을 의미한다. 이 값이 두번째 은닉층의 입력값이 되는데, 첫번째 은닉층에 대한 입력값과 마찬가지로 bias 를 고려해야 하므로 두번째 은닉층의 입력값 $\tilde{\bf{z}}^{(1)}$ 을 다음과 같이 놓자.

$$
\tilde{\bf{z}}^{(1)} := \begin{bmatrix} \bf{z}^{(1)} \\ 1\end{bmatrix} = F^{(1)}\left(\bf{W}^{(1)}\bf{\tilde{x}}\right) = \begin{bmatrix} \sigma_1\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{1}\right) \\ \vdots \\  \sigma_1\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{N_1}\right) \\ 1 \end{bmatrix}
$$

이제 두번째 은닉층에서의 매개변수를 첫번째 은닉층의 매개변수와 같은 방법으로 구성한 $\bf{W}^{(2)}$ 와 두번째 층의 퍼셉트론의 갯수 $N_2$ 에 대해 

$$
\tilde{\bf{z}}^{(2)} = F^{(2)}\left(\bf{W}^{(2)}\tilde{\bf{z}}^{(1)}\right) = \begin{bmatrix}  \sigma_2\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{1}\right) \\ \vdots \\  \sigma_2\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{N_2}\right) \\ 1\end{bmatrix}
$$

라 할 수 있다. 이것을 이후 이어지는 은닉층에 계속 진행 할 수 있다. 임의의 $m$ 번째 은닉층이 $N_m$ 개의 퍼셉트론으로 구성되었다고 하자. 여기에서의 출력값은 은닉층의 매개변수 $\bf{W}^{(m)}$ 과 입력 벡터 $\tilde{\bf{z}}^{(m-1)}$ 에 대해 다음과 같다.

$$
\tilde{\bf{z}}^{(m)} = F^{(m)}\left(\bf{W}^{(m)} \tilde{\bf{z}}^{(m-1)}\right) = \begin{bmatrix} \sigma_m\left(\left[\bf{W}^{(m)} \tilde{\bf{z}}^{(m-1)}\right]_1\right) \\ \vdots \\ \sigma_m\left(\left[\bf{W}^{(m)} \tilde{\bf{z}}^{(m-1)}\right]_{N_m}\right) \\ 1 \end{bmatrix}
$$ {#eq-ML_ANN_output_of_hidden_layer}


이후 [역전파](#sec-ML_ANN_backpropagation) 에 나올 내용이지만 여기서 미리 언급해 두자. 여기서 $\tilde{\bf{z}}^{(m-1)}=\bf{a}$ 에서의 미분은 다음과 같다. 
$$
\dfrac{\partial \tilde{z}_k^{(m)}}{\partial w_{ij}^{(m)}} = \left\{\begin{array}{ll}0, & i\ne k\text{ or } k=N_m+1,\\[0.3em] \sigma'_{m}\left(\left[\bf{W}^{(m)} \bf{a}\right]_i \right)a_j, & i=k \end{array}\right.
$$ {#eq-ML_ANN_derivatives_of_hidden_layer}

즉, 다음과 같이 쓸 수 있다.


$$
\boxed{
\dfrac{\partial \tilde{z}_k^{(m)}}{\partial w_{ij}^{(m)}} = \delta_{ik} \sigma'_{m}\left(\left[\bf{W}^{(m)} \tilde{\bf{z}}^{(m-1)}\right]_i \right)\tilde{z}_j^{(m-1)}
}
$$ {#eq-ML_ANN_derivatives_of_hidden_layer_2}

이제 출력층을 보자. 보통 출력층에서의 활성화 함수는 다른 은닉층의 활성화 함수와 다르기 때문에 $\sigma_O$ 라고 하자. 은닉층이 3개이므로 


$$
\hat{\bf{y}} = \begin{bmatrix}  \sigma_O \left( [\bf{W}^{(O)}\tilde{\bf{z}}^{(3)}]_1\right) \\ \vdots \\\sigma_O \left(  [\bf{W}^{(O)}\tilde{\bf{z}}^{(3)}]_K \right) \end{bmatrix}
$$ {#eq-ML_output_of_mlnn}

이다. 여기에 대한 미분은

$$
\boxed{
\dfrac{\partial \hat{y}_k}{\partial w^{(O)}_{ij}} = \delta_{ik} \sigma_{O}'\left([\bf{W}^{(O)}\tilde{\bf{z}}^{(3)}]_i\right)\tilde{z}^{(3)}_j
}
$${#eq-ML_ANN_derivatives_of_output_layer}

</br>



### 활성화 함수


은닉층, 즉 출력층을 제외한 신경망의 퍼셉트론에 대표적으로 많이 사용되는 활성화 함수에는 다음과 같은 것이 있다. 

![대표적인 활성화 함수](figure/activation_function.png){#fig-ML_activation_functions width=450}

</br>

활성화 함수를 구별짓는 특징으로는 다음과 같은 것들이 있다.

- 함수의 치역(range) 는 어디인가?

- 매끄러운 정도는? 즉 몇번 미분 가능한가?

- 도함수를 원래 함수, 또는 쉽게 계산 할수 있는 함수를 이용하여 쉽게 계산 할 수 있는가?

</br>

이 외에 특정한 목적으로 사용되는 $\text{Maxout}$ 함수가 있다. $\bf{x}=\begin{bmatrix}x_1 & \cdots & x_n\end{bmatrix}^T$ 에 대해 

$$
\text{Maxout}(\bf{x}) := \max \{x_1,\ldots,\,x_n\}
$$ {#eq-ML_maxout}

으로 정의된다.

</br>

### 비용함수 {#sec-ML_cost_function_of_nn}

정해진 모든 퍼셉트론의 파라미터를 $\bf{\theta}$ 라고 하고 입력 $\bf{x}$ 에 대한 출력값을 $\hat{\bf{y}}=F(\bf{x};\bf{\theta})$ 라고 쓰자. 우리가 데이터를 학습시킬 때 $i$ 번째 입력 벡터를 $\bf{x}_{(i)}$ 라고 쓰면 마찬가지로 label 은 원-핫 벡터 $\bf{y}_{(i)}$ 로 표현 할 수 있으며 출력값은 $\hat{\bf{y}}_{(i)}=F\left(\bf{x}_{(i)};\bf{\theta}\right)$ 로 표기 할 수 있다. 다층신경망에 사용된 모든 퍼셉트론의 파라미터를 $\bf{\theta}$ 라고 표기하기로 하자. 다층 신경망에서의 오차함수는 신경망에서의 출력값의 특성에 따라 최소 제곱합이나 교차 엔트로피 함수 혹은 다른 특정한 목적을 위한 함수를 사용 할 수 있다. 


</br>

### 경사 하강법 {#sec-ML_ANN_gradient_discent}

결국은 다층 신경망을 학습시키는 것은 입력 데이터 $\bf{x}^{(1)}, \,\bf{x}^{(2)},\ldots$ 와 정답(label) $\bf{y}^{(1)},\,\bf{y}^{(2)},\ldots$, 모델함수 $f(\bf{x};\bf{\theta})$ 에 대해 비용함수 $L(\bf{\theta})$ 를 최소화 하는 파라미터 $\bf{\theta}$ 를 찾는 문제이다. 문제는 입력 및 정답의 갯수 뿐만 아니라 $\bf{\theta}$ 의 갯수도 아주 크다는 것이다. 어쨌든 신경망은 일단 파라미터에 제한이 없으므로 unrestricted condtion 의 최적화 문제로 볼 수 있다. 이 경우 사용하는 것이 경사하강법이다. 매개변수 $\bf{\theta}$ 는 $k$ 번째 update 에서의 학습률 $\eta_k$ 에 대해 다음과 같이 update 된다.

$$
\bf{\theta}_{k+1} = \bf{\theta}_k - \eta_k \nabla_\bf{\theta}L(\bf{\theta}_k) 
$$

$\nabla_{\bf{\theta}}L(\bf{\theta}_k)$ 를 구하는 방법은 뒤의 [역전파](#sec-ML_ANN_backpropagation)에서 다루기로 하고 일단 이것을 알고 있다고 하자.


한번의 업데이트에 사용되는 데이터셋을 **배치(batch)** 라고 한다. 데이터셋의 크기가 매우 클 경우 모든 데이터셋을 포함하여 비용함수를 계산한다면, 비용함수 및 그 그래디언트를 계산하는데 계산 비용이 많이 소모된다. 만약 소수의 데이터셋만 사용한다면 계산량은 줄지만 노이즈에 취약하게 된다. 이를 위해 전체 데이터셋을 몇등분하며 각각의 등분된 부분의 데이터셋으로 파라미터를 업데이트 할 수 도 있다. 이렇게 등분된 데이터셋을 **미니 배치(mini batch)** 라고 한다. 전체의 데이터셋으로 파라미터를 업데이트 시켰을 경우를 1 **에포크(epoch)** 라고 한다.

즉 1000 개의 훈련용 데이터 셋이 있다면 1000 개의 데이터셋으로 이루어진 1개의 배치가 있다. 이것을 250 개의 미니배치로 나누어 250 개의 데이터셋을 이용하여 4번 파라미터를 업데이트 할 수도 있다. 어쨌든 1000 개의 준비된 데이터셋을 사용하여 파라미터를 업데이트 하였다면 1 에포크 의 훈련 혹은 학습을 수행한 것이다. 

</br>

배치와 미니배치에 따라 세가지 방법을 사용 할 수 있다.

 - **배치 경사 하강법 (Batch gradient discent)** : 각각의 $\bf{\theta}$ 업데이트에 대해 전체 데이터셋을 사용한다. 즉 미니배치를 사용하지 않는다.노이즈에 대해 강건하지만(영향을 적게 받지만) 계산량이 많다.
 - **확률적 경사 하강법 (Stochastic gradient discent)** : 데이터셋 하나가 미니배치를 이룬다. 즉 $\bf{\theta}$ 업데이트에 하나의 데이터셋만 사용한다. 계산량이 가장 적지만 노이즈에 취약하다.
 - **미니 배치 경사 하강법(Mini batch gradient discent)** : 전체 데이터셋보다 작고 하나보다 큰 미니배치를 사용한다.


</br>

## 역전파 (Backpropagation) {#sec-ML_ANN_backpropagation}

역전파는 비용함수 $L(\bf{\theta})$ 의 내부 매개변수가 $\bf{\theta}_k$ 로 주어졌을 때 $\nabla_\bf{\theta}L(\bf{\theta}_k)$ 를 계산하는 방법이다. 역전파를 위해서는 우선 주어진 매개변수 $\bf{\theta}$ 와 입력값에 대해 모든 퍼셉트론에 대해서 출력값이 정해져야 한다. 신경망의 경우 입력 벡터, 혹은 데이터셋에 대해 $\tilde{\bf{z}}^{(1)}$ 부터 마지막 은닉층의 $\tilde{\bf{z}}^{(M)}$ 와 출력값 $\hat{\bf{y}}$ 까지 모두 정해진다. 뿐만 아니라 [자동미분(Auto differentiation)](https://julia-kaeri.github.io/JuliaNumerical/src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#자동-미분)을 통해 각 퍼셉트론의 @eq-ML_ANN_derivatives_of_hidden_layer_2 가 계산된다. 이것을 **순전파 (forward propagation)** 라고 한다. 



그리고 이 비용함수 값과 모든 퍼셉트론의 출력값을 이용하여 신경망에서 $\nabla_\bf{\theta} L(\bf{\theta}_k)$ 를 계산 할 때  의 후진법(Backward method or reverse method) 를 사용하며, 신경망의 각 층을 거꾸로 거슬러 올라가며 $\bf{\theta}$ 를 update 하므로 **역전파(back propagation)** 이라고 불린다.

다음과 같은 신경망을 생각하자.

- $\R^D$ 차원의 입력 벡터와 $\R^K$ 차원의 출력 벡터, 그리고 $M$ 개의 은닉층과 출력층으로 구성되었으며 $l$ 번째 은닉층에는 $N_l$ 개의 퍼셉트론이 존재한다.

</br>


### 출력층에서의 역전파 {#sec-ML_ANN_backpropagation_at_output_layer}

출력층의 매개변수를 $\bf{W}^{(O)}$ 이라고 하고 활성화 함수를 $\sigma_{(O)}$ 라고 하자. 그리고 비용 함수를 $L(\bf{\theta})$ 라고 하자. 그렇다면 비용함수는 $\bf{W}^{(1)},\ldots,\, \bf{W}^{(M)},\, \bf{W}^{(O)}$ 의 함수이다.

$$
L(\bf{W}) = L\left(\bf{W}^{(1)},\ldots,\,\bf{W}^{(M)},\, \bf{W}^{(F)}\right)
$$

Chain rule 을 이용하면 @eq-ML_output_of_mlnn 에 대해 다음을 알 수 있다. 여기서 $\sigma'_{O,\,i}:=\sigma'_{O}\left(\left[\bf{W}^{(O)} \tilde{\bf{z}}^{(M)}\right]_i \right)$ 라고 하면

$$
\dfrac{\partial L(\bf{W})}{\partial w^{(O)}_{ij}} = \sum_k \dfrac{\partial L}{\partial \hat{y}_k} \dfrac{\partial \hat{y}_k}{\partial w^{(O)}_{ij}} = \sum_k \dfrac{\partial L}{\partial \hat{y}_k}\delta_{ik}\sigma'_{O, i} \tilde{z}^{(M)}_j= \dfrac{\partial L}{\partial\hat{y}_i} \dfrac{\partial \hat{y}_i}{\partial w^{(O)}_{ij}}
$$ {#eq-ML_ANN_derivatives_at_output_layer}
 
이다. 기본적으로 자동 미분에 의해 $\dfrac{\partial \hat{y}_i}{\partial w^{(O)}_{ij}}$ 는 순전파 과정에서 이미 정해졌으며 여기서는 ${\partial L}/{\partial \hat{y}_i}$ 만을 구해서 출력층에서의 매개변수에 대한 편미분을 구할 수 있다. 이제 마지막 은닉층에서의 출력값 $\tilde{z}^{(M)}_j= \sigma \left(\left[\bf{W}^{(M)}\tilde{\bf{z}}^{(M-1)}\right]_j\right)$ 에 대해
$$
\dfrac{\partial L(\bf{W})}{\partial w^{(M)}_{ij}} = \sum_{k} \sum_{l}\dfrac{\partial L(\bf{\theta})}{\partial \hat{y}_k} \dfrac{\partial \hat{y}_k}{\partial \tilde{z}^{(M)}_l}\dfrac{\partial \tilde{z}^{(M)}_l}{\partial w^{(M)}_{ij}} 
$${#eq-ML_ANN_derivatives_at_final_hidden_layer_1}

이며, 

$$
\sigma'_{M, k} = \left(\dfrac{d\sigma_M(t)}{dt}\right)_{t=[\bf{W}^{(M)}\tilde{\bf{z}}]_k}
$$ 

라고 하면 
$$
\dfrac{\partial \hat{y}_k}{\partial \tilde{z}_l^{(M)}}=\sigma'_{M, k} w^{(M)}_{kl},\qquad \dfrac{\partial \tilde{z}_l^{(M)}}{\partial w^{(M)}_{ij}}= \delta_{il}\sigma'_{M, i}\tilde{z}^{(M-1)} = \delta_{li}\dfrac{\partial \tilde{z}^{(M)}_l}{\partial w^{(M)}_{ij}} 
$$

이므로 @eq-ML_ANN_derivatives_at_final_hidden_layer_1 은

$$
\dfrac{\partial L}{\partial w^{(M)}_{ij}} = \sum_{k_O} \dfrac{\partial L}{\partial \hat{y}_{k_O}} \sigma'_{M, k} w_{k_O,i}^{(M)} \dfrac{\partial \tilde{z}^{(M)}_i}{\partial w^{(M)}_{ij}} 
$${#eq-ML_ANN_derivatives_at_final_hidden_layer_2}

이다. 여기서 $\dfrac{\partial \tilde{z}^{(M)}_i}{\partial w^{(M)}_{ij}}$ 는 순전파 과정에서 정해지므로 마지막 은닉층의 매개변수에 대한 비용함수의 편미분을 모두 구할 수 있다. 이제 $m$ 번 은닉층의 매개변수에 대한 편미분은

$$
\boxed{
\begin{aligned}
\dfrac{\partial L}{\partial w_{ij}^{(m)}} &= \sum_{k_O=1}^{K} \sum_{k_M=1}^{N_M} \cdots \sum_{k_{m+1}=1}^{N_{m+1}}\dfrac{\partial L}{\partial \hat{y}_{k_O}}  \\[0.3em]
&\qquad \times \left[ \sigma'_{M, k_O} w^{(M)}_{k_O, k_M}\right] \left[\sigma'_{M-1, k_{M-1}}w^{(M-1)}_{k_{M},k_{M-1}}\right] \cdots \left[\sigma'_{m+1,\, k_{m+1}}w^{(m+1)}_{k_{m+1}, i}\right]\dfrac{\partial \tilde{z}^{(m)}_i}{\partial w^{(m)}_{ij}}
\end{aligned}
}
$$ {#eq-ML_ANN_derivatives_at_arbitrary_hidden_layer}

로 구할 수 있다. 

</br>

### 기울기 소멸 {#sec-ML_ANN_vanishing_gradient}

신경망 초기에는 활성화 함수로 sigmoid 함수를 만이 사용했다. $f(t)$ 가 sigmoid 함수일 때 $0<f(t)<1$ 이며

$$
f'(t) = \dfrac{e^{-t}}{(1+e^{-t})^2}= f(t) (1-f(t))
$$

이다. Sigmoid 함수와 그 도함수는 아래 그림과 같으며 도함수의 최대값은 0.25 이다. 

![Sigmoid 함수와 그 미분](figure/sigmoid_and_diff.png){#fig-ML_ANN_sigmoid_function_and_its_derivative}

@eq-ML_ANN_derivatives_at_arbitrary_hidden_layer 를 보면 각 층에서의 활성화 함수의 도함수가 계속 곱해진다. 신경망의 층수가 매우 크다면 입력층에 가까운 퍼셉트론의 매개변수에 대한 미분값은 $0$ 에 가까울 것이며 따라서 경사 하강법에 의헤 업데이트 되는 매개변수 $\theta$ 는 그 update 가 매우 느리거나 멈출 수 밖에 없다. 즉

$$
\theta_{k+1} = \theta_k - \alpha_k \dfrac{\partial L}{\partial \theta}
$$

에서 $\partial L/\partial \theta \approx 0$ 이므로 $\theta_{k+1}\approx \theta_k$ 가 된다. 이 현상을 **기울기 소멸(vanishing gradient)** 라고 한다. 기울기 소멸을 극복하기 위해 다양한 기법이 제시되었으며 대표적으로 활성화 함수를 기울기 소멸이 쉬운 sigmoid 함수나 $\tanh$ 함수를 사용하지 않고 $\text{ReLU}$, 혹은 이에서 파생한 $\text{Leaky ReLU}$, $\text{ELU}$ 함수를 사용하는 방법이 있다(@fig-ML_activation_functions 를 보라).


